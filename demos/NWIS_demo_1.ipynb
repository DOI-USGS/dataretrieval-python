{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National trends in peak annual streamflow\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates a slightly more advanced application of data_retrieval.nwis to collect  using a national dataset of historical peak annual streamflow measurements. The objective is to use a regression of peak annual streamflow and time to identify any trends. But, not for a singile station,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before we begin any analysis, we'll need to setup our environment by importing any modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from dataretrieval import nwis, utils, codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "Recall that the basic way to download data from NWIS is through through the `nwis.get_record()` function, which returns a user-specified record as a `pandas` dataframe. The `nwis.get_record()` function is really a facade of sorts, that allows the user to download data from various NWIS services through a consistant interface. To get started, we require a few simple parameters: a list of site numbers or states codes, a service, and a start date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download annual peaks from a single site\n",
    "df = nwis.get_record(sites='03339000', service='peaks', start='1970-01-01')\n",
    "df.head()\n",
    "\n",
    "# alternatively information for the entire state of illiois can be downloaded using\n",
    "#df = nwis.get_record(state_cd='il', service='peaks', start='1970-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the fields are empty, but no matter. All we require are date (`datetime`), site number (`site_no`), and peak streamflow (`peak_va`).\n",
    "\n",
    "Note that when multiple sites are specified, `nwis.get_record()` will combine `datetime` and `site_no` fields to create a multi-index dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the regression\n",
    "Next we'll define a function that applies ordinary least squares on peak discharge and time.\n",
    "After grouping the dataset by `site_no`, we will apply the regression on a per-site basis. The results from each site, will be returned as a row that includes the slope, y-intercept, r$^2$, p value, and standard error of the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_trend_regression(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #convert datetimes to days for regression\n",
    "    peak_date = df.index\n",
    "    peak_date = pd.to_datetime(df.index.get_level_values(1))\n",
    "    df['peak_d'] = (peak_date - peak_date.min()) / np.timedelta64(1,'D')\n",
    "    #df['peak_d'] = (df['peak_dt'] - df['peak_dt'].min())  / np.timedelta64(1,'D')\n",
    "    \n",
    "    #normalize the peak discharge values\n",
    "    df['peak_va'] = (df['peak_va'] - df['peak_va'].mean())/df['peak_va'].std()\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_error = stats.linregress(df['peak_d'], df['peak_va'])\n",
    "    \n",
    "    #df_out = pd.DataFrame({'slope':slope,'intercept':intercept,'p_value':p_value},index=df['site_no'].iloc[0])\n",
    "    \n",
    "    #return df_out\n",
    "    return pd.Series({'slope':slope,'intercept':intercept,'p_value': p_value,'std_error':std_error})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_trend_analysis(states, start_date):\n",
    "    \"\"\"\n",
    "    states : list\n",
    "        a list containing the two-letter codes for each state to include in the \n",
    "        analysis.\n",
    "    \n",
    "    start_date : string\n",
    "        the date to use a the beginning of the analysis.\n",
    "    \"\"\"\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for state in states:\n",
    "        # download annual peak discharge records\n",
    "        df = nwis.get_record(state_cd=state, start=start_date, service='peaks')\n",
    "        # group the data by site and apply our regression\n",
    "        temp = df.groupby('site_no').apply(peak_trend_regression).dropna()\n",
    "        # drop any insignificant results\n",
    "        temp = temp[temp['p_value']<0.05]\n",
    "        \n",
    "        # now download metadata for each site, which we'll use later to plot the sites\n",
    "        # on a map\n",
    "        site_df = nwis.get_record(sites=temp.index, service='site')\n",
    "        \n",
    "        if final_df.empty:\n",
    "            final_df = pd.merge(site_df, temp, right_index=True, left_on='site_no')\n",
    "            \n",
    "        else:\n",
    "            final_df = final_df.append( pd.merge(site_df, temp, right_index=True, left_on='site_no') )\n",
    "            \n",
    "    return final_df\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the analysis for all states since 1970, one would only need to uncomment and run the following lines. However, pulling all that data from NWIS takes time and puts and could put a burden on resoures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning these lines will download a large dataset from the web and\n",
    "# will take few minutes to run.\n",
    "\n",
    "#start = '1970-01-01'\n",
    "#states = codes.state_codes\n",
    "#final_df = peak_trend_analysis(states=states, start_date=start)\n",
    "#final_df.to_csv('datasets/peak_discharge_trends.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, lets quickly load some predownloaded data, which I generated using the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('datasets/peak_discharge_trends.csv')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the data has been transformed. In addition to statistics about the peak streamflow trends, we've also used the NWIS site service to add latitude and longtitude information for each station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results\n",
    "Finally we'll use `basemap` and `matplotlib`, along with the location information from NWIS, to plot the results on a map (shown below). Stations with increasing peak annual discharge are shown in red; whereas, stations with decreasing peaks are blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently commented out as there isn't an easy way to install mpl_toolkits \n",
    "# on a remote machine without spinning up a full geospatial stack.\n",
    "\n",
    "# from mpl_toolkits.basemap import Basemap, cm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure(num=None, figsize=(10, 6) )\n",
    "\n",
    "# # setup a basemap covering the contiguous United States\n",
    "# m = Basemap(width=5500000, height=4000000, resolution='l',\n",
    "#             projection='aea', lat_1=36., lat_2=44, lon_0=-100, lat_0=40)\n",
    "\n",
    "\n",
    "# # add coastlines\n",
    "# m.drawcoastlines(linewidth=0.5)\n",
    "\n",
    "# # add parallels and meridians.\n",
    "# m.drawparallels(np.arange(-90.,91.,15.),labels=[True,True,False,False],dashes=[2,2])\n",
    "# m.drawmeridians(np.arange(-180.,181.,15.),labels=[False,False,False,True],dashes=[2,2])\n",
    "\n",
    "# # add boundaries and rivers\n",
    "# m.drawcountries(linewidth=1, linestyle='solid', color='k' ) \n",
    "# m.drawstates(linewidth=0.5, linestyle='solid', color='k')\n",
    "# m.drawrivers(linewidth=0.5, linestyle='solid', color='cornflowerblue')\n",
    "\n",
    "\n",
    "# increasing = final_df[final_df['slope'] > 0]\n",
    "# decreasing = final_df[final_df['slope'] < 0]\n",
    "\n",
    "# #x,y = m(lons, lats)\n",
    "\n",
    "# # categorical plots get a little  ugly in basemap\n",
    "# m.scatter(increasing['dec_long_va'].tolist(), \n",
    "#           increasing['dec_lat_va'].tolist(), \n",
    "#           label='increasing', s=2, color='red',\n",
    "#           latlon=True)\n",
    "\n",
    "# m.scatter(decreasing['dec_long_va'].tolist(), \n",
    "#           decreasing['dec_lat_va'].tolist(), \n",
    "#           label='increasing', s=2, color='blue',\n",
    "#           latlon=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2da9d245c691c2995f0592d1f809d130e15c1d01d60d03d4ca8d56ea51bb4095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
