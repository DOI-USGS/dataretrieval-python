{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0ca866",
   "metadata": {},
   "source": [
    "# Using the `waterdata` module to pull data from the USGS Water Data APIs\n",
    "The `waterdata` module will eventually replace the `nwis` module for accessing USGS water data. It leverages the [Water Data APIs](https://api.waterdata.usgs.gov/) to download metadata, daily values, and instantaneous values. \n",
    "\n",
    "While the specifics of this transition timeline are opaque, it is advised to switch to the new functions as soon as possible to reduce unexpected interruptions in your workflow.\n",
    "\n",
    "As always, please report any issues you encounter on our [Issues](https://github.com/DOI-USGS/dataretrieval-python/issues) page. If you have questions or need help, please reach out to us at comptools@usgs.gov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccb6e8",
   "metadata": {},
   "source": [
    "## Prerequisite: Get your Water Data API key\n",
    "We highly suggest signing up for your own API key [here](https://api.waterdata.usgs.gov/signup/) to afford yourself higher rate limits and more reliable access to the data. If you opt not to register for an API key, then the number of requests you can make to the Water Data APIs is considerably lower, and if you share an IP address across users or workflows, you may hit those limits even faster. Luckily, registering for an API key is free and easy.\n",
    "\n",
    "Once you've copied your API key and saved it in a safe place, you can set it as an environment variable in your python script for the current session:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['API_USGS_PAT'] = 'your_api_key_here'\n",
    "``` \n",
    "Note that the environment variable name is `API_USGS_PAT`, which stands for \"API USGS Personal Access Token\".\n",
    "\n",
    "If you'd like a more permanent, repository-specific solution, you can use the `python-dotenv` package to read your API key from a `.env` file in your repository root directory, like this:\n",
    "\n",
    "```python\n",
    "!pip install python-dotenv # only run this line once to install the package in your environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # this will load the environment variables from the .env file\n",
    "```\n",
    "Make sure your `.env` file contains the following line:\n",
    "```\n",
    "API_USGS_PAT=your_api_key_here\n",
    "```\n",
    "Also, do not commit your `.env` file to version control, as it contains sensitive information. You can add it to your `.gitignore` file to prevent accidental commits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b3f0f",
   "metadata": {},
   "source": [
    "## Lay of the Land\n",
    "Now that your API key is configured, it's time to take a 10,000-ft view of the functions in the `waterdata` module.\n",
    "\n",
    "### Metadata endpoints\n",
    "These functions retrieve metadata tables that can be used to refine your data requests.\n",
    "\n",
    "- `get_reference_table()` - Not sure which parameter code you're looking for, or which hydrologic unit your study area is in? This function will help you find the right input values for the data endpoints to retrieve the information you want.\n",
    "- `get_codes()` - Similar to `get_reference_table()`, this function retrieves dataframes containing available input values that correspond to the Samples water quality database.\n",
    "\n",
    "### Data endpoints\n",
    "- `get_daily()` - Daily values for monitoring locations, parameters, stat codes, and more.\n",
    "- `get_continuous()` - Instantaneous values for monitoring locations, parameters, statistical codes, and more.\n",
    "- `get_monitoring_locations()`- Monitoring location information such as name, monitoring location ID, latitude, longitude, huc code, site types, and more.\n",
    "- `get_time_series_metadata()` - Timeseries metadata across monitoring locations, parameter codes, statistical codes, and more. Can be used to answer the question: what types of data are collected at my site(s) of interest and over what time period are/were they collected? \n",
    "- `get_latest_continuous()` - Latest instantaneous values for requested monitoring locations, parameter codes, statistical codes, and more.\n",
    "- `get_latest_daily()` - Latest daily values for requested monitoring locations, parameter codes, statistical codes, and more.\n",
    "- `get_field_measurements()` - Physically measured values (a.k.a discrete) of gage height, discharge, groundwater levels, and more for requested monitoring locations.\n",
    "- `get_samples()` - Discrete water quality sample results for monitoring locations, observed properties, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b5aebf",
   "metadata": {},
   "source": [
    "### A few key tips\n",
    "- You'll notice that each of the data functions have many unique inputs you can specify. **DO NOT** specify too many! Specify *just enough* inputs to return what you need. But do not provide redundant geographical or parameter information as this may slow down your query and lead to errors.\n",
    "- Each function returns a Tuple, containing a dataframe and a Metadata class. If you have `geopandas` installed in your environment, the dataframe will be a `GeoDataFrame` with a geometry included. If you do not have `geopandas`, the dataframe will be a `pandas` dataframe with the geometry contained in a coordinates column. The Metadata object contains information about your query, like the query url.\n",
    "- If you do not want to return the `geometry` column, use the input `skip_geometry=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68591b52",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Let's get into some examples using the functions listed above. First, we need to load the `waterdata` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dataretrieval import waterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcodes,metadata = waterdata.get_reference_table(\"parameter-codes\")\n",
    "display(pcodes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0eab77",
   "metadata": {},
   "source": [
    "Let's say we want to find all parameter codes relating to streamflow discharge. We can use some string matching to find applicable codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ccb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamflow_pcodes = pcodes[pcodes['parameter_name'].str.contains('streamflow|discharge', case=False, na=False)]\n",
    "display(streamflow_pcodes[['parameter_code_id', 'parameter_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9487ee4",
   "metadata": {},
   "source": [
    "Interesting that there are so many different streamflow-related parameter codes! Going on experience, let's use the most common one, `00060`, which is \"Discharge, cubic feet per second\".\n",
    "\n",
    "Now that we know which parameter code we want to use, let's find all the stream monitoring locations that have recent discharge data and at least 10 years of daily values in the state of Nebraska. We will use the `waterdata.get_time_series_metadata()` function to suss out which sites fit the bill. This function will return a row for each *timeseries* that matches our inputs. It doesn't contain the daily discharge values themselves, just information *about* that timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee1da9",
   "metadata": {},
   "source": [
    "First, let's get our expected date range in order. Note that the `waterdata` functions are capable of taking in bounded and unbounded date and datetime ranges. In this case, we want the start date of the discharge timeseries to be no more recent than 10 years ago, and we want the end date of the timeseries to be from at most a week ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_years_ago =(date.today() - relativedelta(years=10))\n",
    "one_week_ago = (datetime.now() - timedelta(days=7)).date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd98164",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_discharge,_ = waterdata.get_time_series_metadata(\n",
    "    state_name=\"Nebraska\",\n",
    "    parameter_code='00060',\n",
    "    begin=f\"1700-01-01/{ten_years_ago}\",\n",
    "    end=f\"{one_week_ago}/..\",\n",
    "    skip_geometry=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(NE_discharge.sort_values(\"monitoring_location_id\").head())\n",
    "print(f\"There are {len(NE_discharge['monitoring_location_id'].unique())} sites with recent discharge data available in the state of Nebraska\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f464470",
   "metadata": {},
   "source": [
    "In the dataframe above, we are looking at 5 timeseries returned, ordered by monitoring location. You can also see that the first two rows show two different kinds of discharge for the same monitoring location: a mean daily discharge timeseries (with statistic id 00003, which represents \"mean\") and an instantaneous discharge timeseries (with statistic id 00011, which represents \"points\" or \"instantaneous\" values). Look closely and you may also notice that the `parent_timeseries_id` column for daily mean discharge matches the `time_series_id` for the instantaneous discharge. This is because once instantaneous measurements began at the site, they were used to calculate the daily mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b8830",
   "metadata": {},
   "source": [
    "Now that we know which sites have recent discharge data, let's find stream sites and plot them on a map. We will use the `waterdata.get_monitoring_locations()` function to grab more metadata about these sites. Even though we have a list of monitoring location IDs from the timeseries function, it's faster to use the `state_name` argument to return all stream sites, and then filter down to the ones we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4df5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_locations,_ = waterdata.get_monitoring_locations(\n",
    "    state_name=\"Nebraska\",\n",
    "    site_type_code=\"ST\"\n",
    "    )\n",
    "\n",
    "NE_locations_discharge = NE_locations.loc[NE_locations['monitoring_location_id'].isin(NE_discharge['monitoring_location_id'].unique().tolist())]\n",
    "display(NE_locations_discharge[[\"monitoring_location_id\", \"monitoring_location_name\", \"hydrologic_unit_code\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe5c4e",
   "metadata": {},
   "source": [
    "If you have `geopandas` installed, the function will return a `GeoDataFrame` with a `geometry` column containing the monitoring locations' coordinates. If you don't have `geopandas` installed, it will return a regular `pandas` DataFrame with coordinate columns instead. Let's take a look at the site locations using `gpd.explore()`. Hover over the site points to see all the columns returned from `waterdata.get_monitoring_locations()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_locations_discharge.set_crs(crs=\"WGS84\").explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d92784",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_sites = NE_locations['monitoring_location_id'].to_list()\n",
    "print(len(ne_sites))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ca5e1",
   "metadata": {},
   "source": [
    "We cannot feed 1700+ monitoring location ID's into the time series metadata function, so we will need to break this list up into smaller chunks and loop through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58307318",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=50\n",
    "chunks = [ne_sites[i:i + chunk_size] for i in range(0, len(ne_sites), chunk_size)]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89379400",
   "metadata": {},
   "source": [
    "Now, we will loop through each chunk of sites and pull timeseries information for sites with discharge data from the past week and a timeseries that is at least 10 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a67e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for site_group in chunks:\n",
    "        try:\n",
    "            timeseries_info,_ = waterdata.get_time_series_metadata(\n",
    "                monitoring_location_id=site_group,\n",
    "                parameter_code='00060',\n",
    "                begin=f\"1700-01-01/{ten_years_ago}\",\n",
    "                end=f\"{one_week_ago}/..\",\n",
    "                skip_geometry=True\n",
    "            )\n",
    "            if not timeseries_info.empty:\n",
    "                dfs = pd.concat([dfs, timeseries_info])\n",
    "        except Exception as e:\n",
    "            # Log & continue; you can also implement retries here\n",
    "            print(f\"Batch failed (size={len(site_group)}): {e}\")\n",
    "\n",
    "display(dfs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b01ca3",
   "metadata": {},
   "source": [
    "One thing you might notice is that this dataframe returns a `state_name` column!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
